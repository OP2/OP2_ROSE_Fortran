\chapter{ FAQ }

\label{faq:introduction}

This chapter accumulates frequently ask questions (FAQ) about ROSE. The questions are not
created by the authors (such FAQs are not particularly useful).

\begin{enumerate}

\item Is ROSE a preprocessor, a translator, or a compiler? \\
   Technically, no! ROSE is formally a meta-tool, a tool for building tools.
{\em ROSE is an object-oriented framework for building source-to-source translators.}
A preprocessor knows nothing of the syntax or semantics of the
language being preprocessed, typically it recognizes another embedded language
within the input file (or attempts to recognize subsets of source language).  In 
contrast, translators process the input language with precision identical to a
compiler.  Since ROSE helps build source-to-source translators, we resist calling
the translators compilers, since the output is not machine code.  This point is
not a required part of the definition of a compiler, many language compilers use
a particular language as an assembly language level (typically {\tt C}).  These
are no less a compiler.  But since we do source-to-source, we feel uncomfortable with
calling the translators compilers (the output language is typically the {\em same} 
as the input language).  The point is further muddled since it is
common in ROSE to have a translator hide the call to the vendor's compiler and thus
the translator can be considered to generate machine code. But this gives little
credit to the vendor's compiler.  So we prefer to refer to our work as a tool 
(or framework) for building source-to-source translators.

\item What does the output from a ROSE translator look like? \\
   A great deal of effort has been made to preserve the quality of your 
original code when regenerated by a translator built using ROSE.
ROSE preserves all formatting, comments, and preprocessor control 
structure.  There are examples in the ROSE Tutorial that make this point clear.

\item How do I debug my transformation? \\
   There are a couple of ways to debug your transformation, but in general
the process starts with knowing exactly what you want to accomplish.
An example of your transformation on a specific input code is particularly useful.
Depending on the type of transformation, there are different mechanisms within 
ROSE to support the development of a transformation.  Available mechanisms
include (in decreasing levels of abstractions):
\begin{enumerate}
   \item String-Based Specification. \\
      A transformation may specify new code to be inserted into the AST
      by specifying the new code as a source code string.  Functions are
      included to permit {\tt insert(), replace(), remove()}.
   \item Calling Predefined Transformations. \\
      There are a number of predefined optimizing transformations (loop optimizations) 
      that may be called directly within a translator built using ROSE.
   \item Explicit AST Manipulation. \\
      The lowest level is to manipulate the AST directly.  Numerous functions
      within SAGE III are provided to support this, but of course it is rather 
      tedious.
\end{enumerate}

\commentout{
\item How do I use the database? \\
   ROSE has a connection to MySQL, but you must run configure with the
correct command-line options to enable it.  Example scripts to configure
ROSE to use MySQL are in the {\tt ROSE/scripts} directory. Another detail 
is that MySQL development generally lags ROSE in the use of the newest 
versions of compilers. So you are likely to be forced to use an older 
version of your compiler (particularly with GNU g++).
}

\item How do I use the SQLite database? \\
   ROSE has a connection to SQLite, but you must run configure with the
correct command-line options to enable it.  Example scripts to configure
ROSE to use SQLite are in the {\tt ROSE/scripts} directory. Another detail 
is that SQLite development generally lags behind ROSE in the use of the newest 
versions of compilers. So you are likely to be forced to use an older 
version of your compiler (particularly with GNU g++).

\item What libraries and include paths do I need to build an application using ROSE. \\
      Run {\tt make installcheck} and observe the command lines used to
      compile the example applications.  These command lines will be what you will
      want to reproduce in your {\tt Makefile}.

\item Where is the {\tt SgTypedefSeq} used? \\
    Any type may be hidden behind a chain of {\em typedefs}. The typedef sequence is the
    list of typedefs that have been applied to any given type.

\item Why are there defining and non-defining declarations? \\

{\indent
{\mySmallFontSize
\begin{verbatim}
     class X;            // non-defining declaration
     X* foo();           // return type of function will refer to non-defining declaration
     X* xPointer = NULL; // Again, the type will refer to a pointer-to-a-type that will be the non-defining declaration.
     class X {};         // defining declaration
\end{verbatim}
}}

The traversal will visit the declarations, so you will, in this case, see the {\tt class X;}
class declaration and the {\tt class X {};} class declaration.  In general, all references to
the class X will use the non-defining declaration, and only the location were X is defined
will be a defining declaration.  This is discussed in great detail in the chapter on SAGE III of 
the ROSE User Manual and a bit in the Doxygen Web pages.

In general, while unparsing, we can't be sure where the definitions associated with
declarations are in the AST (without making the code generation significantly more
complex).
{\indent
{\mySmallFontSize
\begin{verbatim}
     class X;
     class X{};
\end{verbatim}
}}
could be unparsed as:
{\indent
{\mySmallFontSize
\begin{verbatim}
     class X {};  // should have been "class X;"
     class X;     // should have been "class X {};"
\end{verbatim}
}}
The previous example hardly communicates the importance of this concept, but perhaps this
one does:
{\indent
{\mySmallFontSize
\begin{verbatim}
     class X;
     class Y {};
     class X { Y y };
\end{verbatim}
}}
would not compile if unparsed as:
{\indent
{\mySmallFontSize
\begin{verbatim}
     class X { Y y };
     class Y {};
     class X
\end{verbatim}
}}
Note that we can't just make a declaration as being a defining declarations since they are
shared internally (types and symbols can reference them, etc.).



\item Why are comments and CPP directives following the statements being 
      removed and reinserted elsewhere?
      I have been working on a translator, based on the 
      {\tt ROSE/tutorial/CharmSupport.C} translator. If an include statement is in the 
      top of the input code, then the struct added to the top of the source 
      file will contain the include statements in an obviously bad place:
{\indent
{\mySmallFontSize
\begin{verbatim}
     struct AMPI_globals_t
        {
       // A Comment
     #include "stdio.h"
          int a_global_int;
        };
\end{verbatim}
}}
   I am specifying the end of construct for the SgClassDefinition to be 
{\tt Sg\_File\_Info::generateDefaultFileInfoForTransformationNode();} The class 
declaration is prepended into the global scope. How do I correctly 
insert the new definition and declaration into the top of a file(either 
before or after the include statements).

% Thanks,
% Isaac Dooley

The answer, for anyone interested, is found in a discussion relative to the ROSE Tutorial
example (Global Variable Handling, currently Chapter 30).  

The problem is that comments and preprocessor ({\tt cpp}) directives
are attached to the statements. When I wrote the tutorial example
showing how to collect the global variables and put them into a 
data structure, I was not careful to use the low level rewrite mechanism
to do the removal of the variable declarations from the global scope
and the insertion of the same variable declarations into the scope of
the class declaration (the struct that holds the previously global 
variables).  Since the comments and {\tt cpp} directives were attached to
the variable declaration, they got moved with the declaration into the 
new struct that holds them (see the example in the tutorial).

   I should have used the rewrite's mechanism for removing and
reinserting the variable declarations since it is careful to 
disassociate and reassociate comments and cpp directives.
In fact, it is quite incredible that I didn't use that 
slightly higher level interface, because I wrote all that stuff
several years ago and it was so much work to get it all correct.
I'm a big believer in using the highest level of interfaces
possible (which perhaps means I should document them better 
in the Web pages for the IR instead of just in the ROSE User Manual).

The AST Rewrite Mechanism functions to use are the 
{\indent
{\mySmallFontSize
\begin{verbatim}
     LowLevelRewrite::remove ( SgStatement* astNode )
\end{verbatim}
}}
and 
{\indent
{\mySmallFontSize
\begin{verbatim}
     LowLevelRewrite::insert ( SgStatement* targetStatement, SgStatementPtrList newStatementList, bool insertBeforeNode ).
\end{verbatim}
}}

These will automatically disassociate any {\tt cpp} directives and comments
from the surrounding statements and reattach them so that they don't
wander around with the statements being removed, inserted, or replaced.

I will try to get to fixing up the ROSE Tutorial example so use this
interface.  Rich and I have been spending a lot of time on the Tutorial 
lately (after finishing the ROSE User Manual two weeks ago).  We are getting
all the documentation ready for release on the web.  This will likely 
happen in a few weeks, though all the paperwork and approvals are already
in place.

So as it is, this is a wonderful example of just what a bad idea it is
to manipulate the AST at such a low level.
%; using the IR's insert, replace, and remove member functions. 
It is the reason we have the AST Rewrite Mechanism -- provide the highest level
of interface required to make manipulation generally more simple.

% OK, I guess I learned my lesson here :-), back to work, thanks,
% Dan


% DQ (8/7/2008): Added more questions from email on 8/7/2008.
\item  We have read, that the rose compiler is provided under the BSD license. Is every
    part of the rose compiler under BSD licence and is it free for commercial use? \\
   ROSE is free for commercial use, our research license with EDG has no
restrictions (except that we can only release the binary and not the source code).
Obviously the EDG part is not released BSD, only the source code part.
If you want to build products using ROSE for C/C++, then you should
consider contacting EDG for a license to there work then you could
build commercial products and sell them; but you don't have to worry
about ROSE.  I have no idea what ground your on if you build commercial
products for sale based on ROSE and just use the EDG binary
that we provide.  I expect it would be a complicated install for your customers. 
In general if you are using EDG, and building commercial projects for sale,
then I would encourage you to contact EDG and buy a license from them.
This is was a few companies have done, and they have consulted EDG on this
point.  Our goal is to especially encourage open-source C++ work using ROSE.
Clearly we derive robustness in C++ in ROSE from the use of EDG, and we
are thankful to there liberal research license.

\item Is there a list of projects compiled with rose? \\
     I don't release a list of projects and specific research groups using ROSE.  

\item We have read, that you plan a windows port. Until which date do you plan to port the
      project? \\
   We hope to have a windows port using Cygwin, it worked
a while back, but was not tested often, so we have to fix some
details for it to work again.  So it is not a big deal, but I can't
promise when it would happen.

% Questions recieved 9/23/2008 (via Thomas):
\item ROSE computes different kinds of stuff from the actual AST (and semantic/type info): 
      the docs mention control flow, data flow, slicing(?), and some more.
      Are these types of things computed accurately? That is, can you fully rely on the computed info?
      Are they computed for the entire C/C++ language, or a subset? Just to give an
      example: there are implicit calls to destructors of static objects, e.g. f() { A a; }
      will get a \~A call at the end of f()'s scope. Do you take such info into account when
      computing call/dataflow/control graphs? If so, I wonder how you built this info in: do
      you first construct some form of IR (intermediate rep.) atop of which you compute the
      dataflow/call graphs and similar? If so, did you actually add all the 'implicit'
      semantics of C++ manually to the AST? Hope the question is not too unclear.
      How do you handle global static objects?

   The information computed is as accurate as possible and alwasy represent the full
   languge (including full C++, Fortran 2003, etc.).  Some languges are newer
   (e.g. Fortran 2003 and PHP so that will still has to mature).  Implicit calls to
   constructors, destructors, short curcuit evaluation, etc. are not inserted specific analysis in:
   {\tt src/midend/programTransformation/implicitCodeGeneration} is used. This code introduces
   implicit calls into the AST as explicit calls which are ignored by the code generation
   (unparser). \fixme{Check if this code is tested in our regression tests.}.
   Global static objects are not handles specially, but are structurally represented in
   the AST (Note that C++ static constructor evaluation orders are compiler implementation dependent).

\item Linking: to do general full program analysis, you need linking. How did you
    implement this? Did you actually build in all the C/C++ linking semantics by hand?

   We support whole program analysis by permitting the AST's from several files to be
   merged, this saves space in the header file duplication and provides an efficient means
   of handling large scale applications.  This work is currently experemental, and works
   on a 100K C program seperated over 50+ files, but is less robust for C++ code.  It is 
   ongoing research work.  A less scalable alternative is to just list multiple source
   files on the command line, however this is not a meaningful solution for applications
   contianing hundreds or thousands of files.  C++ template details are addressed by
   having each file instantiate all the templates that it requires and then we record
   which of these are used by the file.  All used instantiated templates are represented
   as specialized templates in the AST and any transformed instatiated (specialized)
   templates are output as template specializations, else the backend compiler is used to
   instantiate the required templates so that we can reduce the code generation required.


\item Filtering: say you have a program like \#include <iostream> f() { cout$<<$"x"; }. I
    assume you don't save all the stuff in iostream, and included headers, in your fact
    database - it'll be huge, then. If not, however, you cannot simply discard all stuff
    from system headers, since the user code may refer to them, like, you need the def of
    std::cout in the example above. How do you handle this?
    There also were some remarks in the docs about something like 'sharing' of
    semantically-identical declarations that occur in different parts of the code. Like,
    if you have n declarations of int f(), you would only store one. Is this done within a
    translation unit, or across translation units?

    In the file containing a CPP include directive, the the generated file will be
    essentially identical (i.e. with the CPP include directive).  However, a traversal of the AST
    will include all the items in the include files (and alternative traversal will allow
    you to only travers the input file and skips all other files (e.g. header files).  
    We don't have a database, unless you
    consider the AST as a database (in memory). For the case of {\tt iostream} this will
    be large, but that is what your program really is, so that is how it has to be
    represented; such details are important for type analysis and that trickles into
    every other part of analysis (especially for C++).  The {\bf sharing} is part of
    the support for whole program analysis (global analysis) and it permits redundant 
    parts of the code (e.g. declarations, namespaces, etc.) from being represented
    more than one when handling many files (acorss translation units; tens, hundreds, 
    or thousands).


\item Preprocessing: you mention that ROSE can refer to code locations as they are before
    preprocessing, although it inputs preprocessed files. So, where exactly do you get the
    fine-grained (row,column) info from if you only see the preprocessed files? I assume
    you use \#line directives, but is this really enough (e.g. in the presence of
    whitespace removal by some preprocessors).

    The frontend of EDG includes CPP and thus it reports source code positions
    before the CPP translation, thus we get and save this information.  For Fortran
    we have to handle the CPP translation more explicitly and so we only have the
    source position after translation (but Fortran is always a bit special when it
    is preprocessed).  I am not aware the CPP will remove whitespace, but it is not
    an issue since we get the information from EDG where it is generated before CPP
    translation.

\item Code correctness: say someone analyzes some code which isn't fully correct/complete,
    e.g. misses some includes, or misses some declarations, or plainly has syntax
    errors. What do you do in such a case? Skip somehow the erroneous code, or
    alternatively simply abort?

    We can not currently handl incomplete code, I would argue that any analysis of such
    code would have huge question marks. The essential reasom for this limitation is that 
    we use EDG for C and C++ and it can't handle incomplete code in version 3.4. 
    However, the newer 3.11 version of EDG is expected to handle incomplete code and then
    we will support this, we have no experience with this yet.

\item Dialects: how would you handle different language dialects, e.g. c89,c99,the
    different flavors of C++, Visual C++, etc? Do you build a 'super' grammar that unifies
    all these somehow? Or you have alternative grammars / type checkers?

    We support C89, C99, C++ (98 standard), Fortran 4, Fortran 66, Fortran 77, Fortran 90,
    Fortran 95, Fortran 2003, PHP and Binary Analysis for x86 and ARM using ELF and PE,
    NE, LE, and DOS binary formats).  We will start work on C++0x when we upgrade to the newest
    version of EDG.  We support C++ compiled using Microsoft Visual Studio, but not all
    the MS extensions.  We support a number of GNU specific C and C++ extensions, but not
    all. Since we use EDG for the frontend, we don't have any {\bf super} grammar
    representation (evn EDG does not have such a construction in the design of there
    frontend).  Such concepts don't work well for real languges when you need to handle
    all the corners (which is itself a sad commentary on parser generators and/or modern 
    languages).  For C and C++ the typechecking is mostly done by EDG and we save this
    information and add to it in the ROSE IR.


\end{enumerate}

\commentout{

* Question from Antoine Colin (9/19/2009):
I recently found the ROSE compiler infrastructure and it looks very attractive for my work (building a prototype C++ instrumenter).
The documentation mentions port on cygwin in a few places, but the download links do not seem to point at any?
Is there a version available that can be compiled/linked on cygwin?

Answer from Dan:
   We have internally compiled ROSE on cygwin, and while it
works, we don't try to release a cygwin binary of the EDG work that
would be required for an external release.  So we don't really officially
support it.  I find Cygwin to be so slow that working with ROSE on cygwin
is (I think) a painful process.  So I'm not really supportive of a making cygwin
an official supported platform for ROSE.  I suggest you use a linux or Mac OSX
platform, you will be much happier with the resulting working environment.



* Question about ROSE License policy (submitted to rose-public mailing list week of 9/14/09)



}


\commentout{
Tool Demos at Dagstuhl: Scalable Program Analysis

    * LLNL-ROSE: Daniel J. Quinlan
    * LLVM: Vikram Adve
    * EspC Concurrency Toolset: Jason Yue Yang
    * Parfait: Cristina Cifuentes
    * pluggableTypeChecker: Michael D. Ernst
    * SAFE: Eran Yahav
    * SATIrE: Gergo Barany, Markus Schordan
    * SLAM/SDV,CHESS,HAVOC: Thomas Ball
    * SpaceInvader: Dino Distefano, Hongseok Yang
    * WALA: Steve Fink
    * CodeSonar : David Melski 

 Questions List

    * Which problem does your tool solve?
      ROSE: is a general purpose source-to-source compiler infrastructure for C, C++,
         Fortran 2003, and Binaries; supporting optimization and the connection of
         source code based tools.  By design it supports outside collaborations with 
         so that new research ideas can have impact on DOE applications.  We are obsessed
         with the faithful representation of the source code.
      Compass: is a simple tool wrapping arbitrary static analysis plug-ins for source 
         code and binaries.

    * What kind of analysis is the tool using?
      ROSE: AST, CFG, Call graph, class hierarchy graph, SDG (and you can define your own
         analysis)
      Compass checkers can use any analysis available in ROSE (mostly directly using the
         AST, some using the CFG).

    * What are the strengths/weaknesses of the tool (e.g. which language features are (not) supported?)
      ROSE: high level IR preserves source details, high level IR can be more tedious to
         use for analysis if not lowered or noramlized in a number of ways that are not
         currently supported.
      Compass: Easily extensible to handle new domain-specific or user-defined checkers,
         but only having relativlely simple checkers at present. Plaform for us to
         deliver our research work to users, and support research work on parallel
         program analysis.

    * Usability: What and (qualitatively) how many user annotations does the tool need, if
      any, for (a) primary functionality, and (b) filtering false alarms?
      ROSE requires no annotations as a compiler infrastructure, but supports 
         annotation via comments, pragmas, or in seperate files (Broadway annotation
         mechanism).  Some tools build using ROSE require annotations, but it is
         tool specific.
      Compass: Requires no annotations at present, because none of the existing checkers
         require annotations, but this will likely change in the future.

    * How does your tool interface with other tools?
      ROSE is defined as a library so that it is as easy as possible to use with other
         tools.  How easy and how possible depends on the tool being used. Connections 
         to PAG (SATiR), Yices, Maple, Wave, SPIN, MOPS, MPI, VizAnalyzer, Code Surfer,
         IDA Pro, Objdump, and more.

    * Under what conditions is it available (free download, commercial product)?
      ROSE and Compass are available on the web (BSD), roseCompiler.org and we will
         be moving the SVN repository to outreach.scidac.gov to support working with
         external research groups. (Compass is a project within the ROSE release).
         We use EDG for the C and C++ work and distribute it as a binary.

    * If you were to write your tool from scratch again, what would you do differently?
      ROSE: We rewrite sections from time to time and make significant changes as 
         required, even though it breaks older code.  This defuses some of the desire
         to rewrite anything from scratch.  I wish I had more accurately followed
         the C++ grammar at an earlier stage, since it is difficult to make some 
         changes that we know breack peoples codes.  Wish I had written the (n-1)
         version of ROSETTA (internal IR code generator), instead of making do 
         with the nth version.

(Original questions from each group) Tentative question list

Group 1:

    * What problem your tool is supposed to solve? (For what class of problems your infrastructure can be used?)
    * Who is the target audience? (In what phases of development/maintenance processes is your tool mostly usable?) (What quality attributes of a software are addressed by the tool?)
    * What are the startup costs for using the tool? (Are there any 3rd party dependencies?) (What do you have to learn in order to start using the tool/infrastructure?)
    * Which languages do you handle; do you handle the full language? (What would it mean to add a new language?)
    * False alarms and coverage (precision/recall). (Do you over-approximate or under-approximate?)
    * Do you do static or dynamic analysis, or both?
    * How do you define scalability (what does it mean in the scope of the problem addressed)? (What is the predictable performance, i.e. operating environment (LOC) in which acceptable performance can be gained?)
    * Under what conditions is it available (free download, commercial product)?
    * What are the limitations of the tool/infrastructure?
    * Some words about usability (documentation, GUI, etc.)
    * Did you analyze your tool with your tool? 

Group 2:

For tools:

    * Soundness: Under what conditions are the results of the tool sound? Examples: (In the absence of) Reflection, JNI, user annotations that are not checked 

    * Capabilities: What class of properties can the tool check, and how are the required properties specified? 

    * Usability: What and (qualitatively) how many user annotations does the tool need, if any, for (a) primary functionality, and (b) filtering false alarms? 

For infrastructures:

    * Languages: What source and/or machine languages does the system support? 

    * Representation level: Is the internal representation high-level (AST) or mid-/low-level? The answer should imply whether or not source-level analysis and source-to-source transformations are feasible or not. 

    * Flexibility: What scenarios does the system support? Static analysis? Load-/run-time analysis? Static transformations? Load-/run-time transformations? 

4. Source mapping: Does the system preserve the mapping back to source code after transformations?


For both:

    * License: Under what license(s) is the tool/system available to 

academic, industry research, and industry product users? This question also applies to infrastructures.

    * Support: How well is the tool/system supported in terms of (a) 

technical help for users, and (b) evolution to match evolving languages and systems?

Group 3:

    * Is anyone else using your tool?
          o what is the target audience?
          o How do you convince people that your tool is useful?
          o What are the barriers to adoption? 

    * What kind of expertise or user input is needed?
          o How much can you control or tune the tool?
          o Does the tool require you to change your workflow?
          o If everyone used the tool how would the day-to-day life of programmers be different?
          o How do you design the UI? What are the design trade-offs in the interface? 

    * What kind of analysis are you using?
          o What kind of pre-processing are you doing?
          o When should you stop the analyzer?
          o How do you know that you're not going to scale? 

    * If you could redesign, what would you do differently?
          o What are the caveats of the tool?
                + What are the causes of the false positives? 

Group 4:

    * What is the user interface of the tool/IDE integration?
    * Is it available? How do I get it? What license does it use?
    * How long does it take to get a program through the tool?
    * How many people are using the tool?
    * Why was the tool created? What need does it address?
    * What support is available for the tool?
    * What are the strengths/weaknesses of the tool (e.g. which language features are (not) supported?) 

Group 5:

    * How does the tool explain how it arrived at its results?
    * If you were to rewrite your tool, what would you do differently?
    * What does the tool teach developers about their code, and how can it change their coding practices?
    * Why doesn't your tool interface with other tools? 


}
